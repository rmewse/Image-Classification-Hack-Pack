{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification HackPack\n",
    "\n",
    "In this notebook, we will be constructing a neural network to classify images. We will be training the model on a data set of images. This example is classifying rock, paper, scissors images.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "We will need to install certain python packages before we begin, including: PyTorch, Pandas, MatPlotLib and SkLearn.\n",
    "- Install these packages by opening command line and running: pip install torch pandas matplotlib scikit-learn\n",
    "\n",
    "## Import all required packages\n",
    "\n",
    "Once you have created a new python file, we need to import all of the packages that we have just installed, so that we are able to use them within this project. As well as importing these packages, we will also be importing specific functions etc. from these packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating/finding a dataset\n",
    "\n",
    "For this project, you can either download an existing dataset from online or go one step further and build a custom dataset! 🔥🔥\n",
    "\n",
    "If you choose to work with an existing dataset, some excellent datasets are listed below:\n",
    "\n",
    "MNIST - contains 60,000 training images of handwritten digits 0-9\n",
    "\n",
    "CIFAR-10 - contains 5,000 training images per class (10 classes). Classes consists of animals and everyday items (airplane, cat, horse, truck etc...)\n",
    "\n",
    "CIFAR-100 - similar structure to CIFAR-10, yet contains training images for 100 classes, grouped into 20 superclasses\n",
    "\n",
    "If you choose to create a custom dataset, you will need to source both training and testing photos of each classification (e.g. rock,paper,scissors). To reduce the amount of photos needed, I advise you choose a few simple classes, examples could include:\n",
    "\n",
    "- Pen, Pencil\n",
    "- Different keyboard keys\n",
    "- Facial Expressions\n",
    "- Cups, bottles\n",
    "- Images taken inside, images taken outside\n",
    "- Hand gestures\n",
    "- Different fabrics\n",
    "\n",
    "The more complex classifications you aim to use, the larger your dataset will need to be!\n",
    "To make the dataset effective despite a limited size, consider the tips listed below.\n",
    "\n",
    "- Keep the background simple, if applicable\n",
    "- Use good lighting\n",
    "- Ensure the classes look visually different and are easily distinguishable\n",
    "- Keep the number of photos per class roughly the same\n",
    "- Capture different angles of each class\n",
    "- Use the same pixel dimensions for every image\n",
    "\n",
    "While collecting these images, all images of each classification should be stored in a separate folder.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform values to RGB values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load images for training\n",
    "\n",
    "Here, we need to define the path to the folder containing our training images. This folder should contain individual sub-folders for each class of images.\n",
    "\n",
    "We then define a list for our images, as well as a list of labels. This means that the first label will be the category/ classification of the first image etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"..\" # Path to your dataset\n",
    "\n",
    "image_list = []\n",
    "label_list = []\n",
    "\n",
    "class_list_ = [\"Rock\",\"Paper\",\"Scissors\"] # Our list of classifications\n",
    "\n",
    "for category in class_list_:\n",
    "    for image in os.listdir(f\"{train_data_path}{category}\"):\n",
    "        image_list.append(f\"{train_data_path}{category}/{image}\")\n",
    "        label_list.append(f\"{category}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the data frame\n",
    "\n",
    "We now need to define the data frame that our neural network will use to train. We use the image_list and label_list from before to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "df[\"image\"] = image_list\n",
    "df[\"label\"] = label_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataset \n",
    "\n",
    "Our dataset needs to be split into two sections, images to be used for training and images to be used for training. We can define the ratio that this split will follow.\n",
    "- TIP: You can try adjusting this ratio and see its affect on accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = 0.20 # 20% of data will be for testing\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=ratio, stratify=df[\"label\"], random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmenting data\n",
    "\n",
    "This is where we transform our relatively small data set into a HUGE dataset! This gives the model more data to learn from, leading to higher potential accuracy.\n",
    "\n",
    "Firstly, we define the image size we want to normalise our images to. Using a smaller number will pixelate the images before they are used for training, this removes some data but will allow us to train our model a lot quicker.\n",
    "\n",
    "We then transform the images in both our training and testing sets, applying various different transformations. As previously mentioned, this simulates a much larger data set that we already have!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 28\n",
    "\n",
    "training_transform = transforms.Compose([transforms.Resize((image_size, image_size)),\n",
    "                                            transforms.RandomRotation(10),\n",
    "                                             transforms.ToTensor(),\n",
    "                                         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "test_transform = transforms.Compose([transforms.Resize((image_size, image_size)),\n",
    "                                             transforms.ToTensor(),\n",
    "                                     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Training Data Class\n",
    "\n",
    "This is where the magic begins to happen. We need to define the class that is used for our training data. We can define the BATCH_SIZE to determine the amount of images processed in each batch during training (feel free to adjust this number and see its affect on accuracy).\n",
    "\n",
    "Once we have defined our class, we use it (as well as the batch size) to define our train/ test objects and image loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10\n",
    "\n",
    "class CustomTrainingData(Dataset):\n",
    "    def __init__(self, csv_df, class_list, transform=None):\n",
    "        self.df = csv_df\n",
    "        self.transform = transform\n",
    "        self.class_list = class_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        try:\n",
    "            image_path = self.df.iloc[index][\"image\"]\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            return self.__getitem__((index + 1) % len(self.df)) \n",
    "\n",
    "        label = self.class_list.index(self.df.iloc[index][\"label\"])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    \n",
    "train_data_object = CustomTrainingData(train_df, class_list_, training_transform)\n",
    "test_data_object = CustomTrainingData(test_df, class_list_, test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_data_object, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_data_object, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping\n",
    "\n",
    "While our model is training on our images, it is possible that it begins to simply memorise the training data, rather than learning patterns. This means that the model becomes extremely accurate on training data, but incredibly inaccurate on any new, unseen data. This problem is called \"overfitting\" and leads to significant drops in accuracy. We need to prevent this if we want an accurate model!\n",
    "\n",
    "To prevent this, we define a class below to implement \"early stopping\". Early stopping keeps track of the loss in accuracy for each epoch (round) of training. If the accuracy doesnt improve within 3 rounds (defined by patience=3), we stop the training and save the most accurate model. This prevents the model from beginning to memorise data instead of learning patterns, increasing accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, delta=0):\n",
    "        self.counter = 0\n",
    "        self.best_model_state = None\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.best_model_state = model.state_dict()\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.best_model_state = model.state_dict()\n",
    "            self.counter = 0\n",
    "\n",
    "    def load_best_model(self, model):\n",
    "        model.load_state_dict(self.best_model_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Structure 🔥🔥\n",
    "\n",
    "This is the core of our project; this is where we define HOW data travels through our neural network. We do this by defining the types of layers we are using.\n",
    "\n",
    "There are many types of layers that serve a unique purpose. These layers work together to pass data through the neural network in each round of training, hopefully increasing the accuracy each time. \n",
    "\n",
    "https://www.geeksforgeeks.org/deep-learning/layers-in-artificial-neural-networks-ann/ provides an amazing explanation of the purpose of different types of layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ImageClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, input_size=(image_size, image_size), channels=3): \n",
    "        super(ImageClassifier, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.channels = channels\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(channels, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "\n",
    "        # Batch normalization layers\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "\n",
    "        # Max pooling layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        # Calculate the size of the flattened features\n",
    "        self._to_linear = None\n",
    "        self._calculate_to_linear(input_size)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(self._to_linear, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        \n",
    "    def _calculate_to_linear(self, input_size):\n",
    "        # This function calculates the size of the flattened features\n",
    "        x = torch.randn(1, self.channels, *input_size)\n",
    "        self.conv_forward(x)\n",
    "\n",
    "    def conv_forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
    "\n",
    "        if self._to_linear is None:\n",
    "            self._to_linear = x[0].shape[0] * x[0].shape[1] * x[0].shape[2]\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_forward(x)\n",
    "\n",
    "        # Flatten the output for the fully connected layer\n",
    "        x = x.view(-1, self._to_linear)\n",
    "\n",
    "        # Fully connected layers with ReLU and dropout\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper parameters and initialisation\n",
    "\n",
    "Now that we have defined the layers and structure of our neural network, we need to do some more steps before we define our training loop. Below, we define some hyper parameters that are used in our training loop. \n",
    "- Epochs - The number of \"rounds\" of training\n",
    "- Learning rate - \n",
    "- Num Classes - The number of classifications we have (e.g. rock, paper, scissors)\n",
    "- Input Size - The pixel dimensions of the images we are inputting for training\n",
    "- Channels - Represents the pieces of data for each pixel (3 for RGB)\n",
    "\n",
    "We then check if the program has access to a faster processor (e.g. a GPU), and use this if possible.\n",
    "\n",
    "Finally, we initialise the model using our customised ImageClassifier class!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_CLASSES = len(class_list_)\n",
    "INPUT_SIZE = (image_size, image_size)\n",
    "CHANNELS = 3\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# INIT MODEL\n",
    "model = ImageClassifier(NUM_CLASSES, INPUT_SIZE, CHANNELS).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and early stopping\n",
    "\n",
    "The final step before defining our training loop is to define our loss function and implement or early stopping class from earlier. The loss function is the metric that early stopping will use to decide if our model is making progress or should be halted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOSS FUNCTION\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "early_stopping = EarlyStopping()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THE TRAINING LOOP ❗❗❗\n",
    "\n",
    "We have finally reached the point where we can define the loop that will be train our model, tying together all of our hard work so far! A lot is happening here but below is a summary of each step that this code takes in each epoch of training.\n",
    "\n",
    "- 1. model.train() passes data through the model, attempting to learn patterns to improve accuracy\n",
    "- 2. The loss is calculated for that epoch of training and is printed to the console\n",
    "- 3. The model is evaluated for accuracy\n",
    "- 4. EarlyStopping decides whether the model is making progress or should be halted.\n",
    "- 5. The metrics that describe the current accuracy and performance of the model are calculated and printed to the console.\n",
    "\n",
    "Once the training loop ends - meaning either 100 epochs have run or early stopping has halted the training - the program lets us know by printing \"Training finished!\" to the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{EPOCHS}], Loss: {running_loss/len(train_loader):.4f}')\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        early_stopping(running_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    early_stopping.load_best_model(model)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = 100 * sum(np.array(all_predictions) == np.array(all_labels)) / len(all_labels)\n",
    "    precision = precision_score(all_labels, all_predictions, average='weighted')\n",
    "    recall = recall_score(all_labels, all_predictions, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "\n",
    "    # Print metrics\n",
    "    print(f'Epoch [{epoch+1}/{EPOCHS}]')\n",
    "    print(f'Accuracy on test set: {accuracy:.2f}%')\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Recall: {recall:.4f}')\n",
    "    print(f'F1 Score: {f1:.4f}')\n",
    "\n",
    "print('Training finished!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model\n",
    "\n",
    "Congrats!! You have succesfully built a neural network in python that classifies images! Now we simply save the model and then it can be used to predict the classification of new images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'classifier.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictiion\n",
    "\n",
    "We can now use our model to make predictions on new images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD THE MODEL\n",
    "\n",
    "model = ImageClassifier(num_classes=3, input_size=(128, 128))\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "model.eval()  # Evaluation Mode\n",
    "\n",
    "\n",
    "# PREPARE THE IMAGE\n",
    "\n",
    "img = Image.open('test.jpg')  # Load a new, unseen image from your file system\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "image_tensor = transform(img).unsqueeze(0)\n",
    "\n",
    "# MAKE PREDICTION\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(image_tensor)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    prediction = class_names[predicted.item()]\n",
    "\n",
    "print(f\"Predicted class: {prediction}\") # Outputs the predicted classification of the unseen image"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
